[DEFAULT]
    # Set global limits for current ilifu cluster configuration
    TOTAL_NODES_LIMIT = 79
    CPUS_PER_NODE_LIMIT = 32
    NTASKS_PER_NODE_LIMIT = %(CPUS_PER_NODE_LIMIT)s
    MEM_PER_NODE_GB_LIMIT = 232 #237568 MB
    MEM_PER_NODE_GB_LIMIT_HIGHMEM = 480 #491520 MB
    ACCOUNTS = ['b03-idia-ag','b05-pipelines-ag']

    # Set global values for paths and file names
    LOG_DIR = 'logs'
    CALIB_SCRIPTS_DIR = 'crosscal_scripts'
    AUX_SCRIPTS_DIR = 'aux_scripts'
    SELFCAL_SCRIPTS_DIR = 'selfcal_scripts'
    CONFIG = 'default_config.txt'
    TMP_CONFIG = '.config.tmp'
    MASTER_SCRIPT = 'submit_pipeline.sh'
    SPW_PREFIX = '*:'
    PARTITION = 'Main'
    MODULES = ['openmpi/2.1.1']
    QOS = 'qos-interactive'
    path_binding = ''

    # Set global values for field, crosscal and SLURM arguments copied to config file, and some of their default values
    FIELDS_CONFIG_KEYS = [
        'fluxfield','bpassfield','phasecalfield',
        'targetfields','extrafields'
        ]
    CROSSCAL_CONFIG_KEYS = [
        'minbaselines','chanbin','width','timeavg','createmms',
        'keepmms','spw','nspw','calcrefant','refant','standard',
        'badants','badfreqranges'
        ]
    SELFCAL_CONFIG_KEYS = [
        'nloops','loop','cell','robust','imsize','wprojplanes',
        'niter','threshold','uvrange','nterms','gridder','deconvolver',
        'solint','calmode','discard_nloops','gaintype',
        'outlier_threshold','flag'
        ]
    IMAGING_CONFIG_KEYS = [
        'cell', 'robust', 'imsize', 'wprojplanes', 'niter',
        'threshold', 'multiscale', 'nterms', 'gridder',
        'deconvolver', 'restoringbeam', 'specmode',
        'stokes', 'mask', 'rmsmap'
        ]
    SLURM_CONFIG_STR_KEYS = [
        'container','mpi_wrapper','partition','time','name',
        'dependencies','exclude','account','reservation'
        ]
    SLURM_CONFIG_KEYS_BASE = [
        'nodes','ntasks_per_node','mem','plane','submit',
        'precal_scripts','postcal_scripts','scripts',
        'verbose','modules'
        ]
    CONTAINER = "/idia/software/containers/casa-6.3.simg"
    MPI_WRAPPER = 'mpirun'
    PRECAL_SCRIPTS = [('calc_refant.py',False,''),('partition.py',True,'')] #Scripts run before calibration at top level directory when nspw > 1
    POSTCAL_SCRIPTS = [
        ('concat.py',False,''),
        ('plotcal_spw.py', False, ''),
        ('selfcal_part1.py',True,''),
        ('selfcal_part2.py',False,''),
        ('science_image.py', True, '')
        ] #Scripts run after calibration at top level directory when nspw > 1
    SCRIPTS = [
        ('validate_input.py',False,''),
        ('flag_round_1.py',True,''),
        ('calc_refant.py',False,''),
        ('setjy.py',True,''),
        ('xx_yy_solve.py',False,''),
        ('xx_yy_apply.py',True,''),
        ('flag_round_2.py',True,''),
        ('xx_yy_solve.py',False,''),
        ('xx_yy_apply.py',True,''),
        ('split.py',True,''),
        ('quick_tclean.py',True,'')
        ]
    # sbatch_file_base must use linebreaks for #SBATCH lines! Otherwise python reads them as comments, and ignores them.
    submission_file_base = "#!/bin/bash{array}{exclude}{reservation}\n#SBATCH --account={account}\n#SBATCH --nodes={nodes}\n#SBATCH --ntasks-per-node={tasks}\n#SBATCH --cpus-per-task={cpus}\n#SBATCH --mem={mem}GB\n#SBATCH --job-name={runname}{name}\n#SBATCH --distribution=plane={plane}\n#SBATCH --output={LOG_DIR}/%%x-{ID}.out\n#SBATCH --error={LOG_DIR}/%%x-{ID}.err\n#SBATCH --partition={partition}\n#SBATCH --time={time}"

[ilifu]
    # [DEFAULT] section defined at the top of this file represents the ilifu configuration.

[unknown]
    # Differences to default: memory / node limits are functionally unlimited.
    TOTAL_NODES_LIMIT = 5096
    CPUS_PER_NODE_LIMIT = 1024
    NTASKS_PER_NODE_LIMIT = %(CPUS_PER_NODE_LIMIT)s
    MEM_PER_NODE_GB_LIMIT = 5000 #237568 MB
    MEM_PER_NODE_GB_LIMIT_HIGHMEM = 5000 #491520 MB
    ACCOUNTS = ['']

[galahad]
    # Specify differences to ilifu
    TOTAL_NODES_LIMIT = 17 # Total number of hmem nodes
    CPUS_PER_NODE_LIMIT = 16 # Number of threads on hmem nodes
    NTASKS_PER_NODE_LIMIT = %(CPUS_PER_NODE_LIMIT)s
    MEM_PER_NODE_GB_LIMIT = 1500 # GB; 1.5TB available
    MEM_PER_NODE_GB_LIMIT_HIGHMEM = 1500 # GB; 1.5 TB availble
    ACCOUNTS = [''] # List of allowed accounts; Not currently used: see sbatch_file_base ### update me!!!
    CONTAINER = '/share/nas/mbowles/dev/casa-6.simg' # Previously used '/share/nas/mbowles/mightee/casa-stable.simg'
    PARTITION = 'CLUSTER'
    QOS = 'Normal'
    MPI_WRAPPER = 'mpirun --mca oob tcp'
    MODULES = ['openmpi-2.1.1']
    path_binding = '--bind /share:/share '
    # Must use linebreaks for #SBATCH lines! Otherwise python reads them as comments, and ignores them.
    submission_file_base = "#!/bin/bash{array}{exclude}{reservation}\n#SBATCH --nodes={nodes}\n#SBATCH --ntasks-per-node={tasks}\n#SBATCH --cpus-per-task={cpus}\n#SBATCH --mem={mem}GB\n#SBATCH --job-name={runname}{name}\n#SBATCH --distribution=plane={plane}\n#SBATCH --output={LOG_DIR}/%%x-{ID}.out\n#SBATCH --error={LOG_DIR}/%%x-{ID}.err\n#SBATCH --time={time}"

[petrichor]
    # Specify differences to ilifu
    TOTAL_NODES_LIMIT = 110
    CPUS_PER_NODE_LIMIT = 64
    NTASKS_PER_NODE_LIMIT = %(CPUS_PER_NODE_LIMIT)s
    MEM_PER_NODE_GB_LIMIT = 512 # GB
    MEM_PER_NODE_GB_LIMIT_HIGHMEM = 1000 #
    ACCOUNTS = [''] # List of allowed accounts; Not currently used: see sbatch_file_base ### update me!!!
    CONTAINER = '/scratch1/tho822/containers/casa-pipeline/casa-6.1.2.7-modular.simg' #
    PARTITION = 'defq'
    QOS = 'express'
    MPI_WRAPPER = 'mpirun'
    MODULES = ['singularity', 'use.own', 'openmpi/2.1.1']
    path_binding = '--bind /scratch1:/scratch1,/scratch2:/scratch2 '
    # Must use linebreaks for #SBATCH lines! Otherwise python reads them as comments, and ignores them.
    submission_file_base = "#!/bin/bash{array}{exclude}{reservation}\n#SBATCH --nodes={nodes}\n#SBATCH --ntasks-per-node={tasks}\n#SBATCH --cpus-per-task={cpus}\n#SBATCH --mem={mem}GB\n#SBATCH --job-name={runname}{name}\n#SBATCH --distribution=plane={plane}\n#SBATCH --output={LOG_DIR}/%%x-{ID}.out\n#SBATCH --error={LOG_DIR}/%%x-{ID}.err\n#SBATCH --time={time}"
